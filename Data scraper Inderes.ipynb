{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981b4d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script scrapes links from multiple pages of a website, filters and processes the links, and saves statistical data into separate CSV files.\n",
    "\n",
    "1. Scraping Links:\n",
    "   - The script uses Selenium WebDriver to scrape links from multiple pages of the website.\n",
    "   - It scrolls down each page to load dynamic content and collects all links.\n",
    "   - The collected links are filtered to include only those starting with a specified prefix.\n",
    "\n",
    "2. Processing Links:\n",
    "   - Unique base links are extracted from the filtered links.\n",
    "   - The script splits a list of links into two halves and processes each half separately.\n",
    "   - For each link in the first half:\n",
    "     - If the link does not exist in the DataFrame 'data_df', it fetches HTML code, extracts statistical data, and appends it to the DataFrame.\n",
    "   - For each link in the second half:\n",
    "     - If the link does not exist in the DataFrame 'data_df', it follows the same process as above.\n",
    "\n",
    "3. Saving Data:\n",
    "   - After processing, the statistical data is saved into separate CSV files ('my_datadf500.csv' and 'my_datadf600.csv').\n",
    "   - Before saving, the script checks if the file already exists. If not, it saves the data; otherwise, it skips the save operation.\n",
    "\n",
    "Note: \n",
    "- Ensure the ChromeDriver executable is placed in the correct path for Selenium to work.\n",
    "- Adjust the number of pages to scrape and other parameters as needed.\n",
    "\"\"\"\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "url = 'https://keskustelut.inderes.fi/'\n",
    "html_doc = requests.get(url)\n",
    "list_of_links=[]\n",
    "soup = BeautifulSoup(html_doc.content, 'html.parser')\n",
    "#print(soup.prettify())\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    list_of_links.append(link.get('href'))\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_links = [link for link in list_of_links if link.startswith('https')]\n",
    "filtered_links = filtered_links[2:-1]\n",
    "for link in filtered_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_from_url(url):\n",
    "    \"\"\"\n",
    "    Extracts all links from a given URL.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to scrape for links.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of all extracted links.\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL and store the response\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Find all 'a' tags and extract the 'href' attribute\n",
    "    all_links = [link.get(\"href\") for link in soup.find_all(\"a\", href=True)]\n",
    "    return all_links\n",
    "\n",
    "# Set to store all scraped links\n",
    "scraped_links = set()\n",
    "\n",
    "# Loop through each filtered link and extract links from their pages\n",
    "for link in filtered_links:\n",
    "    extracted_links = extract_links_from_url(link)\n",
    "    scraped_links.update(extracted_links)\n",
    "\n",
    "# Print all scraped links\n",
    "for scraped_link in scraped_links:\n",
    "    print(scraped_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267f812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter links from scraped_links that start with 'https'\n",
    "filtered_links2 = [link for link in scraped_links if link.startswith('https')]\n",
    "filtered_links2 = filtered_links2[2:-1]\n",
    "for link in filtered_links2:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364acd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkkien_joukko = (filtered_links2 + filtered_links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db0f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for links in linkkien_joukko:\n",
    "    print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_set = set()\n",
    "\n",
    "# Add links starting with 'https://keskustelut.inderes.fi' to the new set\n",
    "for link in linkkien_joukko:\n",
    "    if link.startswith('https://keskustelut.inderes.fi'):\n",
    "        filtered_set.add(link)\n",
    "\n",
    "# Print the filtered set\n",
    "for link in filtered_set:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8dc064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_code_of_given_url(url):\n",
    "    \"\"\"\n",
    "    Gets the HTML code of a given URL using Selenium WebDriver.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL to fetch the HTML code from.\n",
    "\n",
    "    Returns:\n",
    "    str: The HTML code of the URL.\n",
    "    \"\"\"\n",
    "    # Path to your ChromeDriver executable\n",
    "    chrome_driver_path = \"C:\\\\Users\\\\mehta\\\\OneDrive\\\\Työpöytä\\\\chromedriver.exe\"\n",
    "\n",
    "\n",
    "\n",
    "    # Set up Chrome options (optional)\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')  # Run Chrome in headless mode (without opening the browser window)\n",
    "\n",
    "    # Create a webdriver instance\n",
    "    service = Service(chrome_driver_path)\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom of the page (or perform the scrolling logic you need)\n",
    "    # Here, it scrolls 30 times with a small delay to load more content\n",
    "    for i in range(30):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Adjust the delay as needed\n",
    "\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Close the browser window\n",
    "    driver.quit()\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd429cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_statistics_from_website(html,data_df,url):\n",
    "    \"\"\"\n",
    "    Extracts statistical data from the HTML code of a website.\n",
    "\n",
    "    Args:\n",
    "    html (str): The HTML code of the website.\n",
    "    data_df (pd.DataFrame): The DataFrame to store the extracted data.\n",
    "    url (str): The URL of the website.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The updated DataFrame with the extracted data.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    number_of_visits = None\n",
    "    created_at = None\n",
    "    replies_count = None\n",
    "    last_reply = None\n",
    "    users_count = None  # Initialize with default value\n",
    "    likes_count = None  # Initialize with default value\n",
    "    links_count = None  # Initialize with default value\n",
    "    comments = None\n",
    "    title = None\n",
    "    visit_element = soup.find('li', class_='secondary views')\n",
    "    if visit_element:\n",
    "        # Find the <span> element within <li> and extract the title attribute\n",
    "        span_element = visit_element.find('span', class_='number')\n",
    "        if span_element and 'title' in span_element.attrs:\n",
    "            number_of_visits = span_element['title']\n",
    "            print(f\"Number of visits: {number_of_visits}\")\n",
    "        else:\n",
    "            print(\"Number of visits not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'secondary views' not found in the HTML content.\")\n",
    "    created_at_element = soup.find('li', class_='created-at')\n",
    "    if created_at_element:\n",
    "        span_element = created_at_element.find('div', class_='topic-map-post created-at')\n",
    "        if span_element:\n",
    "            created_at = span_element.text.strip()\n",
    "            print(f\"Created at: {created_at}\")\n",
    "        else:\n",
    "            print(\"Created-at information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'created-at' not found in the HTML content.\")\n",
    "        created_at = \"\"\n",
    "        # Find the element for replies\n",
    "    replies_element = soup.find('li', class_='replies')\n",
    "    if replies_element:\n",
    "        span_element = replies_element.find('span', class_='number')\n",
    "        if span_element:\n",
    "            replies_count = span_element.text.strip()\n",
    "            print(f\"Replies: {replies_count}\")\n",
    "        else:\n",
    "            print(\"Replies information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'replies' not found in the HTML content.\")\n",
    "        # Find the element for last-reply\n",
    "    last_reply_element = soup.find('li', class_='last-reply')\n",
    "    if last_reply_element:\n",
    "        div_element = last_reply_element.find('div', class_='topic-map-post last-reply')\n",
    "        if div_element:\n",
    "            last_reply = div_element.text.strip()\n",
    "            print(f\"Last Reply: {last_reply}\")\n",
    "        else:\n",
    "            print(\"Last-reply information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'last-reply' not found in the HTML content.\")\n",
    "    users_element = soup.find('li', class_='secondary users')\n",
    "    if users_element:\n",
    "        span_element = users_element.find('span', class_='number')\n",
    "        if span_element:\n",
    "            users_count = span_element.text.strip()\n",
    "            print(f\"Users: {users_count}\")\n",
    "        else:\n",
    "            print(\"Users information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'secondary users' not found in the HTML content.\")\n",
    "\n",
    "    likes_element = soup.find('li', class_='secondary likes')\n",
    "    if likes_element:\n",
    "        span_element = likes_element.find('span', class_='number')\n",
    "        if span_element:\n",
    "            likes_count = span_element.text.strip()\n",
    "            print(f\"Likes: {likes_count}\")\n",
    "        else:\n",
    "            print(\"Likes information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'secondary likes' not found in the HTML content.\")\n",
    "\n",
    "    links_element = soup.find('li', class_='secondary links')\n",
    "    if links_element:\n",
    "        span_element = links_element.find('span', class_='number')\n",
    "        if span_element:\n",
    "            links_count = span_element.text.strip()\n",
    "            print(f\"Links: {links_count}\")\n",
    "        else:\n",
    "            print(\"Links information not found in the HTML content.\")\n",
    "    else:\n",
    "        print(\"Element with class 'secondary links' not found in the HTML content.\")\n",
    "\n",
    "    posts = soup.find_all('article', class_='boxed onscreen-post')\n",
    "    all_posts_comments = []\n",
    "\n",
    "    for post in posts:\n",
    "        try:\n",
    "            post_id = post.get('id')\n",
    "            comments = post.find('div', class_='regular contents')\n",
    "            post_comments_details = []\n",
    "\n",
    "            if comments:\n",
    "                for comment in comments.find_all('p'):\n",
    "                    try:\n",
    "                        \n",
    "                        timestamp = None\n",
    "                        post_div = comment.find_parent('article', class_='boxed onscreen-post')\n",
    "                        if post_div:\n",
    "                            post_info_div = post_div.find('div', class_='post-infos')\n",
    "                            if post_info_div:\n",
    "                                timestamp_element = post_info_div.find('span', class_='relative-date')\n",
    "                                if timestamp_element and 'title' in timestamp_element.attrs:\n",
    "                                    timestamp = timestamp_element['title']\n",
    "\n",
    "                        # Find likes\n",
    "                        likes = 0\n",
    "                        likes_element = comments.find('button', class_='widget-button btn-flat button-count like-count highlight-action regular-likes btn-icon-text')\n",
    "                        if likes_element:\n",
    "                            match = re.match(r'^\\d+', likes_element.text)\n",
    "                            if match:\n",
    "                                likes = int(match.group(0))\n",
    "\n",
    "                        comment_text = comment.text.strip()\n",
    "                        post_comments_details.append({\n",
    "                            \"post_id\": post_id,\n",
    "                            \"comment\": comment_text,\n",
    "                            \"timestamp\": timestamp,\n",
    "                            \"likes\": likes\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing comment in post {post_id}: {e}\")\n",
    "\n",
    "            all_posts_comments.extend(post_comments_details)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post {post_id}: {e}\")\n",
    "            \n",
    "    # Create a new DataFrame with the collected data\n",
    "    new_data = pd.DataFrame({\n",
    "        'URL': [url],\n",
    "        'Title': [title],\n",
    "        'Created At': [created_at],\n",
    "        'Last Reply': [last_reply],\n",
    "        'Visits': [number_of_visits],\n",
    "        'Replies': [replies_count],\n",
    "        'Users': [users_count],\n",
    "        'Likes': [likes_count],\n",
    "        'Links': [links_count],\n",
    "        'Comments Details': [all_posts_comments]\n",
    "    })\n",
    "\n",
    "    # Append the new data to the existing DataFrame\n",
    "    data_df = pd.concat([data_df, new_data], ignore_index=True)\n",
    "\n",
    "    print(f\"Processed URL: {url}\\n\")\n",
    "    return data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26431b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_links_multiple_pages(url, num_pages=40):\n",
    "    \"\"\"\n",
    "    Scrape links from multiple pages by scrolling down and collecting links.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the page to start scraping from.\n",
    "    num_pages (int): The number of pages to scrape.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of links collected from multiple pages.\n",
    "    \"\"\"\n",
    "    # Initialize Chrome webdriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll and wait for dynamic content to load multiple times\n",
    "    for _ in range(num_pages):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Adjust sleep time as needed\n",
    "\n",
    "        # Wait for dynamic content to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "    # Collect all links on the page\n",
    "    all_links = [link.get_attribute(\"href\") for link in driver.find_elements(By.TAG_NAME, \"a\")]\n",
    "\n",
    "    # Quit the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return all_links\n",
    "\n",
    "# Replace this with the URL you want to scrape\n",
    "url_to_scrape = \"https://keskustelut.inderes.fi\"\n",
    "all_links_multiple_pages = scrape_links_multiple_pages(url_to_scrape)\n",
    "\n",
    "# Print the collected links\n",
    "for link in all_links_multiple_pages:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e04867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter links that start with the specified prefix\n",
    "filtered_links = [link for link in all_links_multiple_pages if link.startswith(\"https://keskustelut.inderes.fi/t/\")]\n",
    "\n",
    "# Print the filtered links\n",
    "for link in filtered_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5fc239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty set to store unique base links\n",
    "unique_links = set()\n",
    "\n",
    "# Extract unique base links from filtered_links\n",
    "for link in filtered_links:\n",
    "    # Extract the base link by splitting the URL at the last '/'\n",
    "    base_link = link.rsplit('/', 1)[0]\n",
    "    \n",
    "    # Ensure the base link conforms to the expected pattern of discussion threads\n",
    "    if base_link.startswith('https://keskustelut.inderes.fi/t/'):\n",
    "        # Add the base link to the set\n",
    "        unique_links.add(base_link)\n",
    "\n",
    "# Print the unique base links\n",
    "for link in unique_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31293205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming list_of_links is your list\n",
    "midpoint = len(unique_links) // 2  # Integer division to find the midpoint\n",
    "\n",
    "# Split the list\n",
    "unique_links = list(unique_links)\n",
    "first_half = unique_links[:midpoint]\n",
    "second_half = unique_links[midpoint:]\n",
    "for link in first_half:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6508a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(second_half))\n",
    "data_df = pd.DataFrame(columns=['URL', 'Title', 'Created At', 'Last Reply', 'Visits', 'Replies', 'Users', 'Likes', 'Links', 'Comments Details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda25ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in first_half: \n",
    "    if link not in data_df['URL'].values:\n",
    "        print(link)\n",
    "        html_code = get_html_code_of_given_url(link)\n",
    "        data_df = get_statistics_from_website(html_code, data_df, link)\n",
    "        data_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f9e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('my_datadf5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in second_half:\n",
    "    if link not in data_df['URL'].values:\n",
    "        print(link)\n",
    "        html_code = get_html_code_of_given_url(link)\n",
    "        data_df2 = get_statistics_from_website(html_code, data_df, link)\n",
    "        data_df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df2.to_csv('my_datadf6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
